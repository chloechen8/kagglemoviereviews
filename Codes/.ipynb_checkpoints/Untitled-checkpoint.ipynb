{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open python and nltk packages needed for processing\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.collocations import *\n",
    "from nltk.corpus import sentence_polarity\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'SentimentLexicons/liwcdic2007.dic'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-86-a503b556ca66>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# initialize positve and negative word prefix lists from LIWC\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#   note there is another function isPresent to test if a word's prefix is in the list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;33m(\u001b[0m\u001b[0mposlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneglist\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentiment_read_LIWC_pos_neg_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Downloads\\CIS668\\Term Project\\FinalProjectData\\FinalProjectData\\kagglemoviereviews\\sentiment_read_LIWC_pos_neg_words.py\u001b[0m in \u001b[0;36mread_words\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m   \u001b[0mneglist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m   \u001b[0mflexicon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:/Users/Xin/Downloads/CIS668/Term Project/FinalProjectData/FinalProjectData/kagglemoviereviews/SentimentLexicons/liwcdic2007.dic'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'latin1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m   \u001b[1;31m# read all LIWC words from file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m   \u001b[0mwordlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mflexicon\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'SentimentLexicons/liwcdic2007.dic'"
     ]
    }
   ],
   "source": [
    "import sentiment_read_subjectivity\n",
    "# initialize the positive, neutral and negative word lists\n",
    "(positivelist, neutrallist, negativelist) = sentiment_read_subjectivity.read_subjectivity_three_types('C:/Users/Xin/Downloads/CIS668/Term Project/FinalProjectData/FinalProjectData/kagglemoviereviews/SentimentLexicons/subjclueslen1HLTEMNLP05.tff')\n",
    "\n",
    "import sentiment_read_LIWC_pos_neg_words\n",
    "# initialize positve and negative word prefix lists from LIWC \n",
    "#   note there is another function isPresent to test if a word's prefix is in the list\n",
    "(poslist, neglist) = sentiment_read_LIWC_pos_neg_words.read_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that takes a word and returns true if it consists only\n",
    "#   of non-alphabetic characters  (assumes import re)\n",
    "def alpha_filter(w):\n",
    "  # pattern to match word of non-alphabetical characters\n",
    "  pattern = re.compile('^[^a-z]+$')\n",
    "  if (pattern.match(w)):\n",
    "    return True\n",
    "  else:\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a feature definition function here\n",
    "\n",
    "def document_features(document, word_features):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['V_{}'.format(word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create feature extraction function that has all the word features as before, but also has bigram features.\n",
    "def bigram_document_features(document, word_features, bigram_features):\n",
    "    document_words = set(document)\n",
    "    document_bigrams = nltk.bigrams(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['V_{}'.format(word)] = (word in document_words)\n",
    "    for bigram in bigram_features:\n",
    "        features['B_{}_{}'.format(bigram[0], bigram[1])] = (bigram in document_bigrams)    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding POS tag counts to the word features\n",
    "def POS_features(document):\n",
    "    \tdocument_words = set(document)\n",
    "    \ttagged_words = nltk.pos_tag(document)\n",
    "    \tfeatures = {}\n",
    "    \tfor word in word_features:\n",
    "   \t     features['contains({})'.format(word)] = (word in document_words)\n",
    "    \tnumNoun = 0\n",
    "    \tnumVerb = 0\n",
    "    \tnumAdj = 0\n",
    "    \tnumAdverb = 0\n",
    "    \tfor (word, tag) in tagged_words:\n",
    "    \t    if tag.startswith('N'): numNoun += 1\n",
    "    \t    if tag.startswith('V'): numVerb += 1\n",
    "    \t    if tag.startswith('J'): numAdj += 1\n",
    "    \t    if tag.startswith('R'): numAdverb += 1\n",
    "    \tfeatures['nouns'] = numNoun\n",
    "    \tfeatures['verbs'] = numVerb\n",
    "    \tfeatures['adjectives'] = numAdj\n",
    "    \tfeatures['adverbs'] = numAdverb\n",
    "    \treturn features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix():\n",
    "    goldlist = []\n",
    "    predictedlist = []\n",
    "    for (features, label) in test_set:\n",
    "    \tgoldlist.append(label)\n",
    "    \tpredictedlist.append(classifier.classify(features))\n",
    "    print(\"The confusion matrix is \")\n",
    "    cm = nltk.ConfusionMatrix(goldlist, predictedlist)\n",
    "    print(cm.pretty_format(sort_by_count=True, show_percents=True, truncate=9))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validation:\n",
    "def cross_validation_PRF(num_folds, featuresets, labels):\n",
    "    subset_size = int(len(featuresets)/num_folds)\n",
    "    print('Each fold size:', subset_size)\n",
    "    # for the number of labels - start the totals lists with zeroes\n",
    "    num_labels = len(labels)\n",
    "    total_precision_list = [0] * num_labels\n",
    "    total_recall_list = [0] * num_labels\n",
    "    total_F1_list = [0] * num_labels\n",
    "\n",
    "    # iterate over the folds\n",
    "    for i in range(num_folds):\n",
    "        test_this_round = featuresets[(i*subset_size):][:subset_size]\n",
    "        train_this_round = featuresets[:(i*subset_size)] + featuresets[((i+1)*subset_size):]\n",
    "        # train using train_this_round\n",
    "        classifier = nltk.NaiveBayesClassifier.train(train_this_round)\n",
    "        # evaluate against test_this_round to produce the gold and predicted labels\n",
    "        goldlist = []\n",
    "        predictedlist = []\n",
    "        for (features, label) in test_this_round:\n",
    "            goldlist.append(label)\n",
    "            predictedlist.append(classifier.classify(features))\n",
    "\n",
    "        # computes evaluation measures for this fold and\n",
    "        #   returns list of measures for each label\n",
    "        print('Fold', i)\n",
    "        (precision_list, recall_list, F1_list) \\\n",
    "                  = eval_measures(goldlist, predictedlist, labels)\n",
    "        # take off triple string to print precision, recall and F1 for each fold\n",
    "        '''\n",
    "        print('\\tPrecision\\tRecall\\t\\tF1')\n",
    "        # print measures for each label\n",
    "        for i, lab in enumerate(labels):\n",
    "            print(lab, '\\t', \"{:10.3f}\".format(precision_list[i]), \\\n",
    "              \"{:10.3f}\".format(recall_list[i]), \"{:10.3f}\".format(F1_list[i]))\n",
    "        '''\n",
    "        # for each label add to the sums in the total lists\n",
    "        for i in range(num_labels):\n",
    "            # for each label, add the 3 measures to the 3 lists of totals\n",
    "            total_precision_list[i] += precision_list[i]\n",
    "            total_recall_list[i] += recall_list[i]\n",
    "            total_F1_list[i] += F1_list[i]\n",
    "\n",
    "    # find precision, recall and F measure averaged over all rounds for all labels\n",
    "    # compute averages from the totals lists\n",
    "    precision_list = [tot/num_folds for tot in total_precision_list]\n",
    "    recall_list = [tot/num_folds for tot in total_recall_list]\n",
    "    F1_list = [tot/num_folds for tot in total_F1_list]\n",
    "    # the evaluation measures in a table with one row per label\n",
    "    print('\\nAverage Precision\\tRecall\\t\\tF1 \\tPer Label')\n",
    "    # print measures for each label\n",
    "    for i, lab in enumerate(labels):\n",
    "        print(lab, '\\t', \"{:10.3f}\".format(precision_list[i]), \\\n",
    "          \"{:10.3f}\".format(recall_list[i]), \"{:10.3f}\".format(F1_list[i]))\n",
    "    \n",
    "    # print macro average over all labels - treats each label equally\n",
    "    print('\\nMacro Average Precision\\tRecall\\t\\tF1 \\tOver All Labels')\n",
    "    print('\\t', \"{:10.3f}\".format(sum(precision_list)/num_labels), \\\n",
    "          \"{:10.3f}\".format(sum(recall_list)/num_labels), \\\n",
    "          \"{:10.3f}\".format(sum(F1_list)/num_labels))\n",
    "\n",
    "    # for micro averaging, weight the scores for each label by the number of items\n",
    "    #    this is better for labels with imbalance\n",
    "    # first intialize a dictionary for label counts and then count them\n",
    "    label_counts = {}\n",
    "    for lab in labels:\n",
    "      label_counts[lab] = 0 \n",
    "    # count the labels\n",
    "    for (doc, lab) in featuresets:\n",
    "      label_counts[lab] += 1\n",
    "    # make weights compared to the number of documents in featuresets\n",
    "    num_docs = len(featuresets)\n",
    "    label_weights = [(label_counts[lab] / num_docs) for lab in labels]\n",
    "    print('\\nLabel Counts', label_counts)\n",
    "    #print('Label weights', label_weights)\n",
    "    # print macro average over all labels\n",
    "    print('Micro Average Precision\\tRecall\\t\\tF1 \\tOver All Labels')\n",
    "    precision = sum([a * b for a,b in zip(precision_list, label_weights)])\n",
    "    recall = sum([a * b for a,b in zip(recall_list, label_weights)])\n",
    "    F1 = sum([a * b for a,b in zip(F1_list, label_weights)])\n",
    "    print( '\\t', \"{:10.3f}\".format(precision), \\\n",
    "      \"{:10.3f}\".format(recall), \"{:10.3f}\".format(F1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define features that include word counts of subjectivity words\n",
    "# negative feature will have number of weakly negative words +\n",
    "#    2 * number of strongly negative words\n",
    "# positive feature has similar definition\n",
    "#    not counting neutral words\n",
    "def SL_features(document, word_features, SL):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['V_{}'.format(word)] = (word in document_words)\n",
    "    # count variables for the 4 classes of subjectivity\n",
    "    weakPos = 0\n",
    "    strongPos = 0\n",
    "    weakNeg = 0\n",
    "    strongNeg = 0\n",
    "    for word in document_words:\n",
    "        if word in SL:\n",
    "            strength, posTag, isStemmed, polarity = SL[word]\n",
    "            if strength == 'weaksubj' and polarity == 'positive':\n",
    "                weakPos += 1\n",
    "            if strength == 'strongsubj' and polarity == 'positive':\n",
    "                strongPos += 1\n",
    "            if strength == 'weaksubj' and polarity == 'negative':\n",
    "                weakNeg += 1\n",
    "            if strength == 'strongsubj' and polarity == 'negative':\n",
    "                strongNeg += 1\n",
    "            features['positivecount'] = weakPos + (2 * strongPos)\n",
    "            features['negativecount'] = weakNeg + (2 * strongNeg)      \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One strategy with negation words is to negate the word following the negation word\n",
    "#   other strategies negate all words up to the next punctuation\n",
    "# Strategy is to go through the document words in order adding the word features,\n",
    "#   but if the word follows a negation words, change the feature to negated word\n",
    "# Start the feature set with all 2000 word features and 2000 Not word features set to false\n",
    "def NOT_features(document, word_features, negationwords):\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['V_{}'.format(word)] = False\n",
    "        features['V_NOT{}'.format(word)] = False\n",
    "    # go through document words in order\n",
    "    for i in range(0, len(document)):\n",
    "        word = document[i]\n",
    "        if ((i + 1) < len(document)) and ((word in negationwords) or (word.endswith(\"n't\"))):\n",
    "            i += 1\n",
    "            features['V_NOT{}'.format(document[i])] = (document[i] in word_features)\n",
    "        else:\n",
    "            features['V_{}'.format(word)] = (word in word_features)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute precision, recall and F1 for each label\n",
    "#  and for any number of labels\n",
    "# Input: list of gold labels, list of predicted labels (in same order)\n",
    "# Output:  prints precision, recall and F1 for each label\n",
    "\n",
    "def eval_measures(gold, predicted, labels):\n",
    "    \n",
    "    # these lists have values for each label \n",
    "    recall_list = []\n",
    "    precision_list = []\n",
    "    F1_list = []\n",
    "\n",
    "    for lab in labels:\n",
    "        # for each label, compare gold and predicted lists and compute values\n",
    "        TP = FP = FN = TN = 0\n",
    "        for i, val in enumerate(gold):\n",
    "            if val == lab and predicted[i] == lab:  TP += 1\n",
    "            if val == lab and predicted[i] != lab:  FN += 1\n",
    "            if val != lab and predicted[i] == lab:  FP += 1\n",
    "            if val != lab and predicted[i] != lab:  TN += 1\n",
    "        # use these to compute recall, precision, F1\n",
    "        # for small numbers, guard against dividing by zero in computing measures\n",
    "        if (TP == 0) or (FP == 0) or (FN == 0):\n",
    "          recall_list.append (0)\n",
    "          precision_list.append (0)\n",
    "          F1_list.append(0)\n",
    "        else:\n",
    "          recall = TP / (TP + FP)\n",
    "          precision = TP / (TP + FN)\n",
    "          recall_list.append(recall)\n",
    "          precision_list.append(precision)\n",
    "          F1_list.append( 2 * (recall * precision) / (recall + precision))\n",
    "\n",
    "    # the evaluation measures in a table with one row per label\n",
    "    return (precision_list, recall_list, F1_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use NLTK to compute evaluation measures from a reflist of gold labels\n",
    "#    and a testlist of predicted labels for all labels in a list\n",
    "# returns lists of precision and recall for each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read kaggle training file, train and test a classifier \n",
    "def processkaggle(dirPath,limitStr):\n",
    "  # convert the limit argument from a string to an int\n",
    "  limit = int(limitStr)\n",
    "  \n",
    "  os.chdir(dirPath)\n",
    "  \n",
    "  f = open('./train.tsv', 'r')\n",
    "  # loop over lines in the file and use the first limit of them\n",
    "  phrasedata = []\n",
    "  for line in f:\n",
    "    # ignore the first line starting with Phrase and read all lines\n",
    "    if (not line.startswith('Phrase')):\n",
    "      # remove final end of line character\n",
    "      line = line.strip()\n",
    "      # each line has 4 items separated by tabs\n",
    "      # ignore the phrase and sentence ids, and keep the phrase and sentiment\n",
    "      phrasedata.append(line.split('\\t')[2:4])\n",
    "  \n",
    "  # pick a random sample of length limit because of phrase overlapping sequences\n",
    "  random.shuffle(phrasedata)\n",
    "  phraselist = phrasedata[:limit]\n",
    "\n",
    "  print('Read', len(phrasedata), 'phrases, using', len(phraselist), 'random phrases')\n",
    "\n",
    "  for phrase in phraselist[:10]:\n",
    "    print (phrase)\n",
    "  \n",
    "  # create list of phrase documents as (list of words, label)\n",
    "  phrasedocs = []\n",
    "  # add all the phrases\n",
    "  ###################################################################################\n",
    "  # each phrase has a list of tokens and the sentiment label (from 0 to 4)\n",
    "  ### bin to only 3 categories for better performance\n",
    "  for phrase in phraselist:\n",
    "    tokens = nltk.word_tokenize(phrase[0])\n",
    "    phrasedocs.append((tokens, int(phrase[1])))\n",
    "\n",
    "  # possibly filter tokens\n",
    "  # lowercase - each phrase is a pair consisting of a token list and a label\n",
    "  docs = []\n",
    "  for phrase in phrasedocs:\n",
    "    lowerphrase = ([w.lower() for w in phrase[0]], phrase[1])\n",
    "    docs.append (lowerphrase)\n",
    "  # print a few\n",
    "  for phrase in docs[:10]:\n",
    "    print (phrase)\n",
    "\n",
    "  # possibly filter tokens\n",
    "  stopwords = nltk.corpus.stopwords.words('english')  \n",
    "  #remove stopwords\n",
    "  for phrase in docs:\n",
    "      alphaTokens = [w for w in docs if not alpha_filter(w)]\n",
    "      stopTokens = [w for w in alphaTokens if not stopwords]\n",
    "\n",
    "  # continue as usual to get all words and create word features\n",
    "  all_words_list = [word for (sent,cat) in docs for word in sent]\n",
    "  all_words = nltk.FreqDist(all_words_list)\n",
    "  print(len(all_words))\n",
    "  \n",
    "\n",
    "  bag_of_word = []\n",
    "  # continue as usual to get all words and create word features\n",
    "  for tokens in stopTokens:\n",
    "      t = tokens[0]\n",
    "      for t in tokens:\n",
    "          if t not in bag_of_word:\n",
    "              bag_of_word.append(t)\n",
    "\n",
    "  # get the 1500 most frequently appearing keywords in the corpus\n",
    "  # limit the length of word features to 500\n",
    "  all_words = nltk.FreqDist(bag_of_word)\n",
    "  word_items = all_words.most_common(1500)\n",
    "  word_features = [word for (word, count) in word_items]\n",
    "   \n",
    "  # feature sets from a feature definition function\n",
    "  featuresets = [(document_features(d, word_features), c) for (d, c) in docs]    \n",
    " # feature sets from a feature definition function\n",
    "  print(\"featuresets*********************\")\n",
    "  len(featuresets)\n",
    "\n",
    "  # train classifier and show performance in cross-validation\n",
    "  # make a list of labels\n",
    "  label_list = [c for (d,c) in docs]\n",
    "  labels = list(set(label_list))    # gets only unique labels\n",
    "  num_folds = 5\n",
    "  cross_validation_PRF(num_folds, featuresets, labels)\n",
    "  cross_validation_PRF(num_folds, bigram_featuresets, labels)\n",
    "  cross_validation_PRF(num_folds, POS_featuresets, labels)\n",
    "\n",
    "    \n",
    "  # train classifier and show performance in cross-validation\n",
    "  train_set, test_set = featuresets[1000:], featuresets[:1000]\n",
    "  classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "  print(nltk.classify.accuracy(classifier, test_set)) \n",
    "    \n",
    "    \n",
    "  #creating a short cut variable name for the bigram association measures\n",
    "  bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "  finder = BigramCollocationFinder.from_words(all_words)\n",
    "  bigram_features = finder.nbest(bigram_measures.chi_sq, 500)\n",
    "  print(bigram_features[:50])\n",
    "  \n",
    "    \n",
    "\n",
    "\n",
    "  # bigram_featuresets\n",
    "  print(\"bigram_featuresets*********************\")\n",
    "  bigram_featuresets = [(bigram_document_features(d), c) for (d,c) in documents]\n",
    "  train_set, test_set = bigram_featuresets[1000:], bigram_featuresets[:1000]\n",
    "  classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "  print(nltk.classify.accuracy(classifier, test_set)) \n",
    "\n",
    "  # POS tag features\n",
    "  print(\"POS tag features*********************\")\n",
    "  POS_featuresets = [(POS_features(d, word_features), c) for (d, c) in documents]\n",
    "  # number of features for document 0\n",
    "  len(POS_featuresets[0][0].keys())\n",
    "  # split into training and test and rerun the classifier\n",
    "  train_set, test_set = POS_featuresets[1000:], POS_featuresets[:1000]\n",
    "  classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "  nltk.classify.accuracy(classifier, test_set)\n",
    "\n",
    "  #Subjectivity Lexicon\n",
    "  # create your own path to the subjclues file\n",
    "  print(\"Subjectivity Lexicon*********************\")\n",
    "  SLpath = \"SentimentLexicons/subjclueslen1-HLTEMNLP05.tff\"  \n",
    "  SL = Subjectivity.readSubjectivity(SLpath)\n",
    "  SL_featuresets = [(SL_features(d, word_features, SL), c) for (d, c) in documents]\n",
    "  # retrain the classifier using these features\n",
    "  train_set, test_set = SL_featuresets[1000:], SL_featuresets[:1000]\n",
    "  classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "  nltk.classify.accuracy(classifier, test_set)\n",
    "  \n",
    "  # Negation words\n",
    "  print(\"NOT_featuresets*********************\")\n",
    "  negationwords = ['no', 'not', 'never', 'none', 'nowhere', 'nothing', 'noone', 'rather', 'hardly', 'scarcely', 'rarely', 'seldom', 'neither', 'nor']\n",
    "  # define the feature sets\n",
    "  NOT_featuresets = [(NOT_features(d, word_features, negationwords), c) for (d, c) in documents]\n",
    "  # show the values of a couple of example features\n",
    "  print(NOT_featuresets[0][0]['V_NOTcare'])\n",
    "  print(NOT_featuresets[0][0]['V_always'])\n",
    "  train_set, test_set = NOT_featuresets[1000:], NOT_featuresets[:1000]\n",
    "  classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "  nltk.classify.accuracy(classifier, test_set)\n",
    "  classifier.show_most_informative_features(30)\n",
    "  \n",
    "  # eval_measures(goldlist, predictedlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 156060 phrases, using 100 random phrases\n",
      "[\"is a VH1 Behind the Music special that has something a little more special behind it : music that did n't sell many records but helped change a nation\", '3']\n",
      "['about how lame', '1']\n",
      "[\"'ll put it this way\", '2']\n",
      "[\"I had more fun with Ben Stiller 's Zoolander , which I thought was rather clever .\", '2']\n",
      "['Victor', '2']\n",
      "['The Damned', '2']\n",
      "[\"These guys seem great to knock back a beer with but they 're simply not funny performers .\", '1']\n",
      "[\"It just did n't mean much to me and played too skewed to ever get a hold on -LRB- or be entertained by -RRB-\", '1']\n",
      "['generates by orchestrating a finale that is impenetrable and dull', '0']\n",
      "[\"see a three-dimensional , average , middle-aged woman 's experience\", '2']\n",
      "(['is', 'a', 'vh1', 'behind', 'the', 'music', 'special', 'that', 'has', 'something', 'a', 'little', 'more', 'special', 'behind', 'it', ':', 'music', 'that', 'did', \"n't\", 'sell', 'many', 'records', 'but', 'helped', 'change', 'a', 'nation'], 3)\n",
      "(['about', 'how', 'lame'], 1)\n",
      "([\"'ll\", 'put', 'it', 'this', 'way'], 2)\n",
      "(['i', 'had', 'more', 'fun', 'with', 'ben', 'stiller', \"'s\", 'zoolander', ',', 'which', 'i', 'thought', 'was', 'rather', 'clever', '.'], 2)\n",
      "(['victor'], 2)\n",
      "(['the', 'damned'], 2)\n",
      "(['these', 'guys', 'seem', 'great', 'to', 'knock', 'back', 'a', 'beer', 'with', 'but', 'they', \"'re\", 'simply', 'not', 'funny', 'performers', '.'], 1)\n",
      "(['it', 'just', 'did', \"n't\", 'mean', 'much', 'to', 'me', 'and', 'played', 'too', 'skewed', 'to', 'ever', 'get', 'a', 'hold', 'on', '-lrb-', 'or', 'be', 'entertained', 'by', '-rrb-'], 1)\n",
      "(['generates', 'by', 'orchestrating', 'a', 'finale', 'that', 'is', 'impenetrable', 'and', 'dull'], 0)\n",
      "(['see', 'a', 'three-dimensional', ',', 'average', ',', 'middle-aged', 'woman', \"'s\", 'experience'], 2)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 're' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-bc175f4a5b2d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprocesskaggle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'corpus'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-67-180c6a8e32bb>\u001b[0m in \u001b[0;36mprocesskaggle\u001b[1;34m(dirPath, limitStr)\u001b[0m\n\u001b[0;32m     51\u001b[0m   \u001b[1;31m#remove stopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mphrase\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m       \u001b[0malphaTokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocs\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0malpha_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m       \u001b[0mstopTokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0malphaTokens\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-67-180c6a8e32bb>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     51\u001b[0m   \u001b[1;31m#remove stopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mphrase\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m       \u001b[0malphaTokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocs\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0malpha_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m       \u001b[0mstopTokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0malphaTokens\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-55-916ed889506a>\u001b[0m in \u001b[0;36malpha_filter\u001b[1;34m(w)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0malpha_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m   \u001b[1;31m# pattern to match word of non-alphabetical characters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m   \u001b[0mpattern\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'^[^a-z]+$'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 're' is not defined"
     ]
    }
   ],
   "source": [
    "processkaggle('corpus',100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "commandline interface takes a directory name with kaggle subdirectory for train.tsv\n",
    "   and a limit to the number of kaggle phrases to use\n",
    "It then processes the files and trains a kaggle movie review sentiment classifier.\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if (len(sys.argv) != 3):\n",
    "        print ('usage: classifyKaggle.py <corpus-dir> <limit>')\n",
    "        sys.exit(0)\n",
    "    processkaggle(sys.argv[1], sys.argv[2])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
